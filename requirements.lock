#
# This file is autogenerated by pip-compile with Python 3.12
# by the following command:
#
#    pip-compile --output-file=requirements.lock --strip-extras requirements.in
#
--extra-index-url https://download.pytorch.org/whl/cpu

aiohappyeyeballs==2.6.1
    # via aiohttp
aiohttp==3.13.3
    # via fsspec
aiosignal==1.4.0
    # via aiohttp
annotated-doc==0.0.4
    # via fastapi
annotated-types==0.7.0
    # via pydantic
anyio==4.12.1
    # via
    #   httpx
    #   starlette
attrs==25.4.0
    # via aiohttp
certifi==2026.1.4
    # via
    #   httpcore
    #   httpx
    #   requests
charset-normalizer==3.4.4
    # via requests
click==8.3.1
    # via
    #   typer-slim
    #   uvicorn
datasets==4.5.0
    # via
    #   -r requirements.in
    #   evaluate
dill==0.4.0
    # via
    #   datasets
    #   evaluate
    #   multiprocess
evaluate==0.4.6
    # via -r requirements.in
fastapi==0.128.7
    # via -r requirements.in
filelock==3.20.0
    # via
    #   datasets
    #   huggingface-hub
    #   torch
frozenlist==1.8.0
    # via
    #   aiohttp
    #   aiosignal
fsspec==2025.10.0
    # via
    #   datasets
    #   evaluate
    #   huggingface-hub
    #   torch
h11==0.16.0
    # via
    #   httpcore
    #   uvicorn
hf-xet==1.2.0
    # via huggingface-hub
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via
    #   datasets
    #   huggingface-hub
huggingface-hub==1.4.1
    # via
    #   datasets
    #   evaluate
    #   sentence-transformers
    #   tokenizers
    #   transformers
idna==3.11
    # via
    #   anyio
    #   httpx
    #   requests
    #   yarl
jinja2==3.1.6
    # via torch
joblib==1.5.3
    # via scikit-learn
markupsafe==2.1.5
    # via jinja2
mpmath==1.3.0
    # via sympy
multidict==6.7.1
    # via
    #   aiohttp
    #   yarl
multiprocess==0.70.18
    # via
    #   datasets
    #   evaluate
networkx==3.6.1
    # via torch
numpy==2.4.2
    # via
    #   -r requirements.in
    #   datasets
    #   evaluate
    #   pandas
    #   scikit-learn
    #   scipy
    #   sentence-transformers
    #   transformers
packaging==26.0
    # via
    #   datasets
    #   evaluate
    #   huggingface-hub
    #   transformers
pandas==3.0.0
    # via
    #   datasets
    #   evaluate
propcache==0.4.1
    # via
    #   aiohttp
    #   yarl
pyarrow==23.0.0
    # via datasets
pydantic==2.12.5
    # via fastapi
pydantic-core==2.41.5
    # via pydantic
python-dateutil==2.9.0.post0
    # via pandas
pyyaml==6.0.3
    # via
    #   datasets
    #   huggingface-hub
    #   transformers
regex==2026.1.15
    # via transformers
requests==2.32.5
    # via
    #   datasets
    #   evaluate
safetensors==0.7.0
    # via transformers
scikit-learn==1.8.0
    # via sentence-transformers
scipy==1.17.0
    # via
    #   scikit-learn
    #   sentence-transformers
sentence-transformers==5.2.2
    # via -r requirements.in
shellingham==1.5.4
    # via huggingface-hub
six==1.17.0
    # via python-dateutil
starlette==0.52.1
    # via fastapi
sympy==1.14.0
    # via torch
threadpoolctl==3.6.0
    # via scikit-learn
tokenizers==0.22.2
    # via transformers
torch==2.10.0+cpu
    # via
    #   -r requirements.in
    #   sentence-transformers
tqdm==4.67.3
    # via
    #   datasets
    #   evaluate
    #   huggingface-hub
    #   sentence-transformers
    #   transformers
transformers==5.1.0
    # via
    #   -r requirements.in
    #   sentence-transformers
typer-slim==0.21.1
    # via
    #   huggingface-hub
    #   transformers
typing-extensions==4.15.0
    # via
    #   aiosignal
    #   anyio
    #   fastapi
    #   huggingface-hub
    #   pydantic
    #   pydantic-core
    #   sentence-transformers
    #   starlette
    #   torch
    #   typer-slim
    #   typing-inspection
typing-inspection==0.4.2
    # via
    #   fastapi
    #   pydantic
urllib3==2.6.3
    # via requests
uvicorn==0.40.0
    # via -r requirements.in
xxhash==3.6.0
    # via
    #   datasets
    #   evaluate
yarl==1.22.0
    # via aiohttp

# The following packages are considered to be unsafe in a requirements file:
# setuptools
